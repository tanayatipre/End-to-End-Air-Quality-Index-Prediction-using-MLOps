name: CI/CD Pipeline

on:
  push:
    branches:
      - main
    paths-ignore:
      - 'README.md'
      - 'docs/**' # Ignore documentation changes if any
  # Manual trigger for full retraining/deployment (if desired)
  workflow_dispatch:
    inputs:
      full_train:
        description: 'Run full ML pipeline training and deploy new model'
        required: true # Set to true to require explicit confirmation
        type: boolean
        default: true # Default to true for easier manual triggering

permissions:
  id-token: write # Needed for aws-actions/configure-aws-credentials and OIDC with AWS
  contents: read  # Needed for actions/checkout and reading repository content

env: # Global environment variables for the workflow, automatically picked up by MLflow Python client
  MLFLOW_TRACKING_URI: https://dagshub.com/${{ github.repository }}.mlflow 
  MLFLOW_TRACKING_USERNAME: ${{ secrets.DAGSHUB_USERNAME }} # Your DagsHub username secret
  MLFLOW_TRACKING_PASSWORD: ${{ secrets.DAGSHUB_TOKEN }} # Your DagsHub token secret (recommended over DAGSHUB_PASSWORD)

jobs:
  # --- Phase 1: Fast Continuous Integration (CI) ---
  # Runs linting, unit tests, and caches dependencies. This should be fast on every push.
  continuous-integration:
    name: Lint, Test, and Cache Dependencies
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9' # Or your specific Python version

      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-python-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-python-

      - name: Install dependencies
        run: pip install -r requirements.txt # Installs base requirements

      - name: Lint code
        run: echo "TODO: Implement linting tools like pylint or flake8 here." 

      - name: Run unit tests
        run: echo "TODO: Implement your Python unit tests (e.g., using pytest) here." 


  # --- Phase 2: Full ML Model Training (Conditional Execution) ---
  # This job runs the main ML pipeline to train and evaluate the model.
  # It is triggered ONLY by a manual workflow_dispatch event with 'full_train' checked.
  full-model-training:
    name: Full ML Pipeline Training
    runs-on: ubuntu-latest # Consider 'self-hosted' if training is very resource-intensive
    needs: continuous-integration # Ensures CI checks pass before training starts
    # CRITICAL: This 'if' condition ensures this job runs ONLY on manual trigger with 'full_train' checked
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.full_train == true

    outputs: # Define outputs to pass data (specifically the MLflow run_id) to subsequent jobs
      run_id: ${{ steps.run_pipeline.outputs.run_id }} # Capture run_id from main.py execution

    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-python-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-python-
            
      - name: Install dependencies (including mlflow[s3] and dagshub for tracking)
        run: pip install -r requirements.txt mlflow[s3] dagshub

      # Run the full ML Pipeline and capture the MLflow Run ID.
      # The main.py script will train the model, evaluate it, and log artifacts to MLflow (DagsHub).
      - name: Run Full ML Pipeline Training
        id: run_pipeline # Give this step an ID to access its outputs
        run: |
          echo "Running main.py to train the model..."
          python main.py
          
          # Fetch the latest MLflow Run ID. This assumes main.py completes successfully.
          # We query the local MLflow backend to get the most recent run ID.
          LATEST_RUN_ID=$(mlflow runs list --max-results 1 --output-as json | jq -r '.[0].run_id')
          echo "Captured latest MLflow Run ID: $LATEST_RUN_ID"
          echo "run_id=$LATEST_RUN_ID" >> "$GITHUB_OUTPUT" # Set output for the job, for other jobs to consume

  # --- Phase 3: Build and Push Docker Image (Leveraging MLflow Artifacts) ---
  # This job builds a Docker image that includes the trained model and preprocessor artifacts
  # downloaded from MLflow (DagsHub) using the captured run_id.
  build-and-push-ecr-image:
    name: Build & Push Docker Image
    needs: full-model-training # This job depends on the full training completion
    runs-on: ubuntu-latest # Typically runs on a GitHub-hosted runner for Docker builds
    env: # Pass MLflow Run ID and DagsHub credentials as environment variables for the Docker build context
      MLFLOW_RUN_ID: ${{ needs.full-model-training.outputs.run_id }}
      GITHUB_REPOSITORY: ${{ github.repository }} # e.g., tanayatipre/End-to-End-Machine-Learning-Project-with-MLFlow

    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Install Utilities (jq for JSON parsing, unzip for general utility)
        run: |
          sudo apt-get update
          sudo apt-get install -y jq unzip 

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build, tag, and push image to Amazon ECR
        id: build-image
        env: # Define Docker image specific environment variables
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY: ${{ secrets.ECR_REPOSITORY_NAME }}
          IMAGE_TAG: latest
        run: |
          # Build a docker container.
          # Pass MLflow configuration (URI, credentials) and the specific RUN_ID as build arguments.
          # These build args are then used by the entrypoint.sh script (which calls download_ml_artifacts.py)
          # inside the container at startup.
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \
            --build-arg MLFLOW_RUN_ID=$MLFLOW_RUN_ID \
            --build-arg MLFLOW_TRACKING_URI=${{ env.MLFLOW_TRACKING_URI }} \
            --build-arg MLFLOW_TRACKING_USERNAME=${{ env.MLFLOW_TRACKING_USERNAME }} \
            --build-arg MLFLOW_TRACKING_PASSWORD=${{ env.MLFLOW_TRACKING_PASSWORD }} \
            .
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          echo "::set-output name=image::$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" # Old way to set output

  # --- Phase 4: Continuous Deployment (CD) ---
  # Deploys the newly built Docker image to your self-hosted runner.
  continuous-deployment:
    name: Deploy to Self-Hosted Runner
    needs: build-and-push-ecr-image # Depends on the Docker image being built and pushed
    runs-on: self-hosted # This job runs on your self-hosted machine
    steps:
      - name: Configure AWS credentials (if needed for pulling from ECR on self-hosted runner)
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
          
      - name: Login to Amazon ECR
        id: login-ecr-self-hosted
        uses: aws-actions/amazon-ecr-login@v1
      
      - name: Pull latest image from ECR
        run: |
          docker pull ${{secrets.AWS_ECR_LOGIN_URI}}/${{ secrets.ECR_REPOSITORY_NAME }}:latest

      - name: Stop and remove previous container (if running)
        run: |
          # Stop and remove the old container gracefully if it exists.
          # '|| true' ensures the step doesn't fail if the container isn't found.
          docker ps -a --filter "name=mlproj" --format "{{.ID}}" | xargs -r docker stop || true
          docker ps -a --filter "name=mlproj" --format "{{.ID}}" | xargs -r docker rm -fv || true

      - name: Run New Docker Image to serve users
        run: |
          # Run the new Docker container.
          # Pass MLflow configuration and the specific MLflow Run ID as environment variables to the container.
          # The entrypoint.sh inside the container will use these to download artifacts.
          docker run -d -p 8080:8080 --name=mlproj \
            -e 'MLFLOW_TRACKING_URI=${{ env.MLFLOW_TRACKING_URI }}' \
            -e 'MLFLOW_TRACKING_USERNAME=${{ env.MLFLOW_TRACKING_USERNAME }}' \
            -e 'MLFLOW_TRACKING_PASSWORD=${{ env.MLFLOW_TRACKING_PASSWORD }}' \
            -e 'MLFLOW_RUN_ID=${{ needs.full-model-training.outputs.run_id }}' \
            -e 'ML_ARTIFACTS_DIR=/app/artifacts/downloaded_model' \ # This must match DOWNLOADED_ARTIFACTS_DIR in predictions.py and download_ml_artifacts.py
            ${{secrets.AWS_ECR_LOGIN_URI}}/${{ secrets.ECR_REPOSITORY_NAME }}:latest

      - name: Clean previous images and containers (optional, careful with this if you need old images for rollback)
        run: |
          docker system prune -f || true # Prune unused Docker objects. '|| true' prevents failure.
