name: CI/CD Pipeline

on:
  push:
    branches:
      - main
    paths-ignore:
      - 'README.md'
      - 'docs/**' 
  workflow_dispatch:
    inputs:
      full_train:
        description: 'Run full ML pipeline training and deploy new model'
        required: true 
        type: boolean
        default: false

permissions:
  id-token: write 
  contents: read  

env: 
  MLFLOW_TRACKING_URI: https://dagshub.com/${{ github.repository }}.mlflow 
  MLFLOW_TRACKING_USERNAME: ${{ secrets.DAGSHUB_USERNAME }} 
  MLFLOW_TRACKING_PASSWORD: ${{ secrets.DAGSHUB_TOKEN }} 

jobs:
  # --- NEW DIAGNOSTIC JOB (Optional, keep for initial testing, remove later) ---
  diagnose_trigger:
    name: Diagnose Workflow Trigger
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v3
      - name: Print Trigger Info
        run: |
          echo "GitHub Event Name: ${{ github.event_name }}"
          echo "Full Train Input: ${{ github.event.inputs.full_train }}"

  # --- Phase 1: Fast Continuous Integration (CI) ---
  continuous-integration:
    name: Lint, Test, and Cache Dependencies
    needs: diagnose_trigger # Depends on diagnostic job for initial testing
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9' 

      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-python-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-python-
            
      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Lint code
        run: echo "Linting repository (e.g., pylint, flake8)" 

      - name: Run unit tests
        run: echo "Running unit tests (e.g., pytest)" 


  # --- Phase 2: Full ML Model Training (Conditional) ---
  full-model-training:
    name: Full ML Pipeline Training
    runs-on: ubuntu-latest
    needs: continuous-integration 
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.full_train == true

    outputs: 
      run_id: ${{ steps.run_pipeline.outputs.run_id }} 

    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-python-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-python-
            
      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run Full ML Pipeline Training
        id: run_pipeline 
        run: |
          python main.py
          # Capture the latest MLflow Run ID from the current run
          LATEST_RUN_ID=$(mlflow runs list --max-results 1 --output-as json | jq -r '.[0].run_id')
          echo "Captured latest MLflow Run ID: $LATEST_RUN_ID"
          echo "run_id=$LATEST_RUN_ID" >> "$GITHUB_OUTPUT" # Set job output for run_id

  # --- Phase 3: Docker Image Build and Push ---
  build-and-push-ecr-image:
    name: Build & Push Docker Image
    needs: full-model-training # Depends on successful model training for run_id
    runs-on: ubuntu-latest
    
    # Pass MLflow Run ID and DagsHub credentials as build arguments
    # The GITHUB_REPOSITORY is used to construct the full MLflow Tracking URI in Dockerfile
    # These are passed to the Dockerfile to enable artifact download during build.
    env:
      MLFLOW_RUN_ID_BUILD_ARG: ${{ needs.full-model-training.outputs.run_id }}
      DAGSHUB_USERNAME_BUILD_ARG: ${{ secrets.DAGSHUB_USERNAME }}
      DAGSHUB_TOKEN_BUILD_ARG: ${{ secrets.DAGSHUB_TOKEN }}
      GITHUB_REPOSITORY_BUILD_ARG: ${{ github.repository }}

    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Install Utilities
        run: |
          sudo apt-get update
          sudo apt-get install -y jq unzip 

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build, tag, and push image to Amazon ECR
        id: build-image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY: ${{ secrets.ECR_REPOSITORY_NAME }}
          IMAGE_TAG: latest
        run: |
          # Build a docker container, passing MLflow credentials and run_id as build-args
          # The Dockerfile uses these ARGs to set ENV vars and run download_ml_artifacts.py
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \
            --build-arg MLFLOW_RUN_ID="${{ env.MLFLOW_RUN_ID_BUILD_ARG }}" \
            --build-arg GITHUB_REPOSITORY="${{ env.GITHUB_REPOSITORY_BUILD_ARG }}" \
            --build-arg MLFLOW_TRACKING_USERNAME="${{ env.DAGSHUB_USERNAME_BUILD_ARG }}" \
            --build-arg MLFLOW_TRACKING_PASSWORD="${{ env.DAGSHUB_TOKEN_BUILD_ARG }}" \
            .
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          echo "::set-output name=image::$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" 

  # --- Phase 4: Continuous Deployment ---
  continuous-deployment:
    name: Deploy to Self-Hosted Runner
    needs: build-and-push-ecr-image # Depends on successful image build
    runs-on: self-hosted 
    steps:
      - name: Configure AWS credentials (if needed for pulling from ECR on self-hosted)
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
          
      - name: Login to Amazon ECR
        id: login-ecr-self-hosted
        uses: aws-actions/amazon-ecr-login@v1
      
      - name: Pull latest image
        run: |
          docker pull ${{secrets.AWS_ECR_LOGIN_URI}}/${{ secrets.ECR_REPOSITORY_NAME }}:latest

      - name: Stop and remove previous container (if running)
        run: |
          docker ps -a --filter "name=mlproj" --format "{{.ID}}" | xargs -r docker stop || true
          docker ps -a --filter "name=mlproj" --format "{{.ID}}" | xargs -r docker rm -fv || true

      - name: Run New Docker Image to serve users
        run: |
          # Pass MLflow environment variables to the running container
          # The ML_ARTIFACTS_DIR is already set in the Dockerfile, so no need to pass here
          docker run -d -p 8080:8080 --name=mlproj \
            -e 'MLFLOW_TRACKING_URI=${{ env.MLFLOW_TRACKING_URI }}' \
            -e 'MLFLOW_TRACKING_USERNAME=${{ env.MLFLOW_TRACKING_USERNAME }}' \
            -e 'MLFLOW_TRACKING_PASSWORD=${{ env.MLFLOW_TRACKING_PASSWORD }}' \
            -e 'MLFLOW_RUN_ID=${{ needs.full-model-training.outputs.run_id }}' \
            ${{secrets.AWS_ECR_LOGIN_URI}}/${{ secrets.ECR_REPOSITORY_NAME }}:latest

      - name: Clean previous images and containers (optional, careful with this)
        run: |
          docker system prune -f || true # Use || true to prevent job failure if prune fails
